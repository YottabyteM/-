# 分类

**二分类模型：**

逻辑回归和Fisher线性判别分析

**多分类模型：**

多分类线性判别分析和多分类逻辑回归

对于因变量为二分类变量的情况，我们可以使用逻辑回归进行处理。 把y看成事件发生的概率，y $\leq$ 0.5表示不发生；y>0.5表示发生

**线性概率模型：**

直接使用原来的回归模型进行回归。

$$
y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+...+\beta_kx_{ki}+\mu_i
$$

写成向量乘积形式： 
$y_i=x_i' \beta+u_i$    $(i=1,2,...n)$

但是预测值却可能出现>1或者<0的不现实情况

**两点分布：**

在给定x的情况下，考虑y的两点分布概率

$$
\begin{cases}
P(y=1|x)=F(x,\beta)\\
P(y=0|x)=1-F(x,\beta)
\end{cases}
$$

注：一般
$F(x,\beta)=F(x_i'\beta)$

我们只需要保证 $F(x,\beta)$ 为值域在[0,1]上的函数

其中连接函数 $F(x,\beta)$ 有两种取法：

1.可以取为标准正态分布的累计密度函数cdf:

$$
F(x,\beta)=\psi(x_i'\beta)=\int_{-\infty}^{x_i'\beta} \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}dt
$$

上面为probit回归

2.$F(x,\beta)$可以取为Sigmoid函数：

$$
F(x,\beta)=S(x_i'\beta)=\frac{exp(x'_i\beta)}{1+exp(x'_i\beta)}
$$

上面为logistic回归

由于后者有解析表达式，而标准正态分布的cdf没有，所以计算logistic模型比probit模型更为方便

对于logistic回归，为非线性模型，可以使用极大似然估计方法进行估计

$$
f(y_i|x_i,\beta)=[S(x_i'\beta)]^{y_i}[1-S(x_i'\beta)]^{1-y_i}
$$

取对数后：

$$
lnf(y_i|x_i,\beta)=y_iln[S(x_i'\beta)]+(1-y_i)ln[1-S(x_i'\beta)]
$$

样本的对数似然函数：

$$
lnL(\beta|y,x)=\sum_{i=1}^n y_iln[S(x_i'\beta)]+\sum_{i=1}^n(1-y_i)ln[1-S(x_i'\beta)]
$$

可以使用梯度下降求解这个非线性最大化的问题

在用于分类时，如果 $0.5\leq \hat{y_i}$ ，则认为其预测的y=1；否则则认为预测的y=0

如果采用logistic回归预测结果较差，可以在回归模型中加入平方项、交互项等

**注意：**

加入太多项后，虽然预测能力提高了，但是容易发生过拟合的现象。即对于样本数据的预测非常好，但是对于样本外 的数据的预测效果可能会很差。

解决方案：把数据分为训练组和测试组，用训练组的数据来估计出模 型，再用测试组的数据来进行测试。（训练组和测试组的比 例一般设置为80%和20%)

### Fisher线性判别分析

该方法思想比较简单:给定训练集样例，设法将样例投影到一维的直线上，使得同类样例的投影点尽可能接近和密集，异类投影点尽可能远离。

将所有的点分割在$\omega^Tx=0$两侧，将每个点投影到$\omega$这个法向量上，保证：类内小，类间大

### 多分类问题

Fisher判别分析可以用于多分类

Logistic回归也可用于多分类，将连接函数推广为Softmax函数

# 聚类

**分类是已知类别的，聚类未知。**

### K-means聚类算法

一、指定需要划分的簇[cù]的个数K值（类的个数）; 

二、随机地选择K个数据对象作为初始的聚类中心 （不一定要是我们的样本点）; 

三、计算其余的各个数据对象到这K个初始聚类中心 的距离，把数据对象划归到距离它最近的那个中心所 处在的簇类中; 

四、调整新类并且重新计算出新类的中心; 

五、循环步骤三和四，看中心是否收敛（不变），如果收敛或达到迭代次数则停止循环; 

六、结束

**优点：** 

（1）算法简单、快速。 

（2）对处理大数据集，该算法是相对高效率的。

**缺点：** 

（1）要求用户必须事先给出要生成的簇的数目K。 

（2）对初值敏感。 

（3）对于孤立点数据敏感。

### K-means++算法

k-means++算法选择初始聚类中心的基本原则是：**初始的聚类中心之间的相互距离要尽可能的远**

**算法描述如下：** 

（只对K-means算法“初始化K个聚类中心” 这一步进行了优化） 

步骤一：随机选取一个样本作为第一个聚类中心； 

步骤二：计算每个样本与当前已有聚类中心的最短距离（即与最 近一个聚类中心的距离），这个值越大，表示被选取作为聚类中 心的概率较大；最后，用轮盘法（依据概率大小来进行抽选）选 出下一个聚类中心； 

步骤三：重复步骤二，直到选出K个聚类中心。选出初始点后，就继续使用标准的K-means算法了。

**K-means算法的一些讨论**

1.聚类的个数K值怎么定？

分几类主要取决于个人的经验与感觉，通常的做法是多尝试几个K值， 看分成几类的结果更好解释，更符合分析目的等

2.数据的量纲不一致怎么办?

如果数据的量纲不一样，那么算距离时就没有意义。例如：如果X1 单位是米，X2单位是吨，用距离公式计算就会出现“米的平方”加上“吨的平方” 再开平方，最后算出的东西没有数学意义，这就有问题了。

### 系统（层次）聚类

系统聚类的合并算法通过计算两类数据点间的距离，对最为接近的两类数据点进行组合，并反复迭代这一过程，直到将所有数据点合成一类，并生成聚类谱系图。

系统（层次）聚类算法流程

一、将每个对象看作一类，计算两两之间的最小距离； 

二、将距离最小的两个类合并成一个新类；

 三、重新计算新类与所有类之间的距离； 

四、重复二三两步，直到所有类最后合并成一类； 

五、结束

### 用图形估计聚类的数量

肘部法则（Elbow Method）：通过图形大致的估计出最优的聚类数量

各个类畸变程度之和：各个类的畸变程度等于该类重心与其内部成员位置距离的平方和；

假设一共将n个样本划分到K个类中（K $\leq$ n-1，即至少有一类中有两个元素）

用$C_k$表示第k个类，且该类重心位置记为$u_k$

那么第k个类的畸变程度为：$\sum_{i \in C_k}|x_i-u_k|^2$

定义所有类的总畸变程度：$J=\sum_{k=1}^K\sum_{i \in C_k}|x_i-u_k|^2$

J也被称为聚合系数

聚合系数折线图：横坐标为类别数K，纵坐标为聚合系数J

### DBSCAN算法

DBSCAN(Density-based spatial clustering of applications  with noise)是Martin Ester, Hans-PeterKriegel等人于1996年提出 的一种基于密度的聚类方法，聚类前不需要预先指定聚类的个数，生成的簇的个数不定（和数据有关）。该算法利用基于密度的聚类的概念，即要求聚类空间中的一定区域内所包 含对象（点或其他空间对象）的数目不小于某一给定阈值。 该方法能在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能有效处理异常数据。

DBSCAN算法将数据点分为三类：

核心点：在半径Eps内含有不少于MinPts数目的点 

边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内 

噪音点：既不是核心点也不是边界点的点

**聚类和分类在数学建模中的应用**

数学建模问题可以分为4类：分类问题、优化问题、预测问题、评价问题

对于分类问题，可以采取判别分析、聚类分析、神经网络分类

**判别分析：**（就是上面的分类）

又称“分辨法”，是在分类确定的条件下，根据某一研究对象的各种[特征值](https://link.zhihu.com/?target=https%3A//baike.sogou.com/lemma/ShowInnerLink.htm%3FlemmaId%3D116086%26ss_c%3Dssc.citiao.link)判别其类型归属问题的一种[多变量](https://www.zhihu.com/search?q=多变量&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"408405168"})统计分析方法。

其基本原理是按照一定的判别准则，建立一个或多个[判别函数](https://www.zhihu.com/search?q=判别函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"408405168"})；用研究对象的大量资料确定判别函数中的待定系数，并计算判别指标；据此即可确定某一样本属于何类。当得到一个新的样品数据，要确定该样品属于已知类型中哪一类，这类问题属于判别分析问题。

**[聚类分析](https://www.zhihu.com/search?q=聚类分析&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"408405168"})：**（就是上面的聚类）

聚类分析或聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集，这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。

聚类分析本身不是某一种特定的算法，而是一个大体上的需要解决的任务。它可以通过不同的算法来实现，这些算法在理解集群的构成以及如何有效地找到它们等方面有很大的不同。

**神经网络分类：**

BP 神经网络是一种神经网络学习算法。其由输入层、中间层、输出层组成的阶层型神经网络，中间层可扩展为多层。RBF（径向基）神经网络：径向基函数(RBF-Radial Basis Function)神经网络是具有单隐层的三层前馈网络。它模拟了人脑中局部调整、相互覆盖接收域的神经网络结构。感知器神经网络：是一个具有单层计算神经元的神经网络，网络的传递函数是线性阈值单元。主要用来模拟人脑的感知特征。[线性神经网络](https://www.zhihu.com/search?q=线性神经网络&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"408405168"})：是比较简单的一种神经网络，由一个或者多个[线性神经元](https://www.zhihu.com/search?q=线性神经元&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"408405168"})构成。采用线性函数作为传递函数，所以输出可以是任意值。自组织神经网络：自组织神经网络包括自组织竞争网络、自组织特征映射网络、学习向量量化等网络结构形式。K[近邻算法](https://www.zhihu.com/search?q=近邻算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"408405168"})：　K最近邻分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。

